<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3N1JWQ22YH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3N1JWQ22YH');
    </script>

    <meta charset="utf-8">
    <title>MIT MLSys Discussion Group</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Charles Jin">
    <link href="assets/css/reset.css" rel="stylesheet">
    <link href="assets/css/index.css" rel="stylesheet">
    <script language="javascript">
      window.onload = function () {
        var ul = document.querySelector('ul');
        for (var i = ul.children.length; i >= 0; i--) {
            ul.appendChild(ul.children[Math.random() * i | 0]);
        }
      };
    </script>
  </head>

  <body>
    <div id="main" class="container">
      <p id="title">
        <a href="http://www.mlsys.ai">MIT MLSys Discussion Group</a> </br></br>
      </p>
    </div>

    <div class="container">
      <h1>About</h1>
      <div>
        <p>
          We are a group of MIT students enthusiastic about machine learning systems.
          We sit together weekly to discuss papers that propose influential or impactful ideas for building machine learning systems.
          We generate and record our exchange, ideas and questions here.
          We are currently at capacity and cannot accomodate new members.
        </p>
      </div>

    <div class="container">
      <h1>Active Members</h1><br>
      The following active members contribute to the discussion notes recorded below.

      <div class="group">
        <ul>
          <li><a href="http://www.tjin.org">Tian Jin</a></li>
          <li><a href="https://minimario.github.io">Alex Gu</a></li>
          <li><a href="http://people.csail.mit.edu/yuka/">Yuka Ikarashi</a></li>
          <li><a href="https://twitter.com/exists_forall">William Brandon</a></li>
          <li><a href="https://www.linkedin.com/in/zhiye-song/">Zhiye (Zoey) Song</a></li>
          <li><a href="https://www.anneouyang.com/">Anne Ouyang</a></li>
          <li><a href="https://anin.dev/">Aniruddha (Ani) Nrusimha</a></li>
          <li>Kevin Qian</li>
          <li><a href="https://www.vvhuang.com/">Vincent Huang</a></li>
        </ul><br>
        * List order is randomized upon page load. For inquiries and suggestions, contact Tian Jin &lt;tianjin@[three-letter institute name].edu&gt;.
      </div>
    </div>

    <div class="container">
      <h1>Past Discussions</h1>
      <div class="group">
        <p>
          <b>Cramming: Training a Language Model on a Single GPU in One Day</b><br>
          <b>Summary.</b> Can you pretrain a BERT model on a single 2080ti GPU in 1 day?
          The authors suggest yes.
          The authors find that parameter count is the deciding factor for predicting model performance in low compute resource regime and recommend training/data/architecture modifications to improve pretraining in the low compute resource regime.<br>
          <a href="papers/cramming.html">[Discussion]</a>
        </p>
        <p>
          <b>Efficiently Modeling Long Sequences with Structure</b><br>
          <b>Summary.</b> The authors use state space models, a well-known concept in control theory, to address long range dependencies.
          The challenge lies in making state space models work effectively and efficiently.
          The proposed solution beats transformer models of the same size on long range arena, and has desirable properties such as faster autoregressive generation (than transformer) and the ability to handle sampling resolution change for continuous signals without retraining.<br>
          <a href="papers/S4.html">[Discussion]</a>
        </p>

        <p>
          <b>Decentralized Training of Foundation Models in Heterogeneous Environments</b><br>
          <b>Summary.</b> How to crowd-source training of large language models, especially when network conditions of nodes in the compute network is highly heterogeneous?
          The author formalized this problem by modeling the network communication cost of decentralized training, and proposed a solution that minimizes the communication overhead.
          The proposed solution show impressive performance results: 3.8-4.8x faster than SOTA alternatives designed for homogeneous network conditions.<br>
          <a href="papers/decentralized.html">[Discussion]</a>
        </p>

        <p>
          <b>Training Compute-Optimal Large Language Models</b><br>
          <b>Summary.</b> Existing large language models are under-trained.
          The author empirically investigates how to optimally scale model size and training data size, resulting in Chinchilla -- a model that matches the performance of a 280B parameter model (Gopher) with an order of magnitude smaller size.<br>
          <a href="papers/chinchilla.html">[Discussion]</a>
        </p>

        <p>
          <b>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</b><br>
          <b>Summary.</b> The paper discusses strategies (mostly on partitioning models across different GPUs) and considerations (mostly on avoiding commutations between GPUs) that enables distributed training of an extremely large language models by contemporary standards (8 billion parameters). This work influenced training system designs for recent large language models.<br>
          <a href="papers/megatron.html">[Discussion]</a>
        </p>

        <p>
          <b>Improving language models by retrieving from trillions of tokens</b><br>
          <b>Summary.</b> When generating outputs with language models, Retro searches and retrieves tokens from a database based on similarities with its input in the embedding space. Retro encodes and then incorporate the retrieved text into the intermediate representations of the language model via cross attention. The result is potentially to increase memory capacity of language model without significantly increasing the number of parameters.<br>
          <a href="papers/retro.html">[Discussion]</a>
        </p>

        <p>
          <b>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</b><br>
          <b>Summary.</b> FlashAttention is an exact optimization to the original attention module. The core idea is to compute the NxN (N=sequence len) attention matrix in small tiles such that each tile easily fit within the fast but small memory (SRAM) on GPU. The benefit is that 1) doing so reduces access to the slow but large memory (HBM) thus improving runtime and 2) the full attention is never fully materialized thus improving memory efficiency.<br>
          <a href="papers/flash_attention.html">[Discussion]</a>
        </p>

        <p>
          <b>Monarch: Expressive Structured Matrices for Efficient and Accurate Training</b><br>
          <b>Summary.</b> This paper proposes to transform dense matrix into factors of block sparse diagonal matrices (interspersed with permutation matrices) that 1) have fewer parameters than dense models and 2) can run faster than dense models. This paper is an important episode in the recent development of butterfly-matrix-inspired sparsity patterns that aims to accelerate training with sparsity, which used to be impossible without accuracy degradations. <br>
          <a href="papers/monarch.html">[Discussion]</a>
        </p>
      </div>
    </div>

    <div class="container">
      <h1>P.S.</h1>
      <div class="group">
        <p>We thank <a href="https://people.csail.mit.edu/mcarbin/">Prof. Mike Carbin</a> for providing the funding for this group.</p>
        <p>If you are looking for MLSys conference, visit <a href="https://www.mlsys.org">this link.</a></p>
        <p>We thank <a href="https://charlesjin.com/">Charles Jin</a> for providing this website template.
      </div>
    </div>
  </body>
</html>
