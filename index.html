<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>MIT MLSys Discussion Group</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Charles Jin">
    <link href="assets/css/reset.css" rel="stylesheet">
    <link href="assets/css/index.css" rel="stylesheet">
    <script language="javascript">
      window.onload = function () { 
        var ul = document.querySelector('ul');
        for (var i = ul.children.length; i >= 0; i--) {
            ul.appendChild(ul.children[Math.random() * i | 0]);
        }
      };
    </script>
  </head>

  <body>
    <div id="main" class="container">
      <p id="title">
        <a href="http://www.mlsys.ai">MIT MLSys Discussion Group</a> </br></br>
      </p>
    </div>

    <div class="container">
      <h1>About</h1>
      <div>
        <p>
          We are a group of MIT students enthusiastic about machine learning systems.
          We sit together weekly to discuss papers that propose influential or impactful ideas for building machine learning systems.
          We generate and record our exchange, ideas and questions here.
        </p>
      </div>

    <div class="container">
      <h1>Active Members</h1><br>
      The following active members contribute to the discussion notes recorded below.

      <div class="group">
        <ul>
          <li><a href="http://www.tjin.org">Tian Jin</a></li>
          <li><a href="https://minimario.github.io">Alex Gu</a></li>
          <li><a href="http://people.csail.mit.edu/yuka/">Yuka Ikarashi</a></li>
          <li><a href="https://twitter.com/exists_forall">William Brandon</a></li>
          <li><a href="https://www.linkedin.com/in/zhiye-song/">Zhiye (Zoey) Song</a></li>
          <li><a href="https://www.anneouyang.com/">Anne Ouyang</a></li>
          <li><a href="https://anin.dev/">Aniruddha (Ani) Nrusimha</a></li>
          <li>Kevin Qian</li>
          <li><a href="https://www.vvhuang.com/">Vincent Huang</a></li>
        </ul><br>
        * List order is randomized upon page load. For inquiries and suggestions, contact Tian Jin &lt;tianjin@[three-letter institute name].edu&gt;.
      </div>
    </div>

    <div class="container">
      <h1>Past Discussions</h1>
      <div class="group">
        <p>
          <b>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</b><br>
          <b>Summary.</b> The paper discusses strategies (mostly on partitioning models across different GPUs) and considerations (mostly on avoiding commutations between GPUs) that enables distributed training of an extremely large language models by contemporary standards (8 billion parameters). This work influenced training system designs for recent large language models.<br>
          <a href="papers/megatron.html">[Discussion]</a>
        </p>

        <p>
          <b>Improving language models by retrieving from trillions of tokens</b><br>
          <b>Summary.</b> When generating outputs with language models, Retro searches and retrieves tokens from a database based on similarities with its input in the embedding space. Retro encodes and then incorporate the retrieved text into the intermediate representations of the language model via cross attention. The result is potentially to increase memory capacity of language model without significantly increasing the number of parameters.<br>
          <a href="papers/retro.html">[Discussion]</a>
        </p>

        <p>
          <b>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</b><br>
          <b>Summary.</b> FlashAttention is an exact optimization to the original attention module. The core idea is to compute the NxN (N=sequence len) attention matrix in small tiles such that each tile easily fit within the fast but small memory (SRAM) on GPU. The benefit is that 1) doing so reduces access to the slow but large memory (HBM) thus improving runtime and 2) the full attention is never fully materialized thus improving memory efficiency.<br>
          <a href="papers/flash_attention.html">[Discussion]</a>
        </p>

        <p>
          <b>Monarch: Expressive Structured Matrices for Efficient and Accurate Training</b><br>
          <b>Summary.</b> This paper proposes to transform dense matrix into factors of block sparse diagonal matrices (interspersed with permutation matrices) that 1) have fewer parameters than dense models and 2) can run faster than dense models. This paper is an important episode in the recent development of butterfly-matrix-inspired sparsity patterns that aims to accelerate training with sparsity, which used to be impossible without accuracy degradations. <br>
          <a href="papers/monarch.html">[Discussion]</a>
        </p>
      </div>
    </div>

    <div class="container">
      <h1>P.S.</h1>
      <div class="group">
        <p>We thank <a href="https://people.csail.mit.edu/mcarbin/">Prof. Mike Carbin</a> for providing the funding for this group.</p>
        <p>If you are looking for MLSys conference, visit <a href="https://www.mlsys.org">this link.</a></p>
        <p>We thank <a href="https://charlesjin.com/">Charles Jin</a> for providing this website template.
      </div>
    </div>
  </body>
</html>
